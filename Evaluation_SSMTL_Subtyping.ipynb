{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load subtype label and predicted result of each patch.\n",
    "#### Note that only subtype label are provided for subtyping task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load patch list with corresponding labels - subtype label\n",
    "test_set = pd.read_csv(\"/home5/hby/subtype_newdata/0.3/res_testset.txt\", header=None, sep=' ')\n",
    "test_set.columns = ['filename','label']\n",
    "#test_set = test_set.merge(test_list, how='left', on='slidename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corresponding predicted results\n",
    "predict_all = pd.read_csv(\"/home5/hby/subtype_newdata/0.3/result/tcga.csv\", header=None)\n",
    "predict_all.columns = ['cancer','normal','ccrcc','prcc', 'chrcc']\n",
    "predict_all.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1688822, 5)\n",
      "(1688822, 2)\n"
     ]
    }
   ],
   "source": [
    "# check the patch list length is same or not with predicted list\n",
    "print(predict_all.shape)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            filename  label  cancer  normal  \\\n",
      "0  /home1/gzy/Subtype/Patches/2000/TCGA-B0-5692-0...      0  1.9121 -1.9150   \n",
      "1  /home1/gzy/Subtype/Patches/2000/TCGA-B0-5692-0...      0 -1.5974  1.6026   \n",
      "2  /home1/gzy/Subtype/Patches/2000/TCGA-B0-5692-0...      0  1.2498 -1.2465   \n",
      "3  /home1/gzy/Subtype/Patches/2000/TCGA-B0-5692-0...      0  1.8097 -1.8122   \n",
      "4  /home1/gzy/Subtype/Patches/2000/TCGA-B0-5692-0...      0  1.6865 -1.6885   \n",
      "\n",
      "    ccrcc    prcc   chrcc                                          slidename  \n",
      "0  2.7109 -2.1903 -2.0744  TCGA-B0-5692-01Z-00-DX1_34477dae-21a5-45fc-b94...  \n",
      "1  0.4500 -0.1379 -0.7369  TCGA-B0-5692-01Z-00-DX1_34477dae-21a5-45fc-b94...  \n",
      "2  2.2468 -2.1179 -1.3085  TCGA-B0-5692-01Z-00-DX1_34477dae-21a5-45fc-b94...  \n",
      "3  2.6621 -2.0862 -1.9377  TCGA-B0-5692-01Z-00-DX1_34477dae-21a5-45fc-b94...  \n",
      "4  2.9301 -2.0802 -2.2329  TCGA-B0-5692-01Z-00-DX1_34477dae-21a5-45fc-b94...  \n"
     ]
    }
   ],
   "source": [
    "# concat two list\n",
    "test_set = pd.concat([test_set, predict_all], axis=1)\n",
    "test_set['slidename'] = test_set['filename'].apply(lambda x: x.split('/')[-2])\n",
    "print(test_set.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax targets\n",
    "crds = ['cancer','normal']\n",
    "subtypes = ['ccrcc','prcc','chrcc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_crd = np.array(predict_all[crds])\n",
    "predict_crd_norm = np.zeros_like(predict_crd)\n",
    "for i in range(predict_crd.shape[0]):\n",
    "    predict_crd_norm[i,:] = np.exp(predict_crd[i,:])/sum(np.exp(predict_crd[i,:]))\n",
    "test_set[crds] = predict_crd_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_results = np.array(predict_all[subtypes])\n",
    "predicted_results_norm = np.zeros_like(predicted_results)\n",
    "for i in range(predicted_results.shape[0]):\n",
    "    predicted_results_norm[i,:] = np.exp(predicted_results[i,:])/sum(np.exp(predicted_results[i,:]))\n",
    "test_set[subtypes] = predicted_results_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only cancer patches for aggregation of subtyping prediction.\n",
    "test_set_cancer = test_set[test_set['normal'] < 0.5].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report the evaluation metrics for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two ways to do aggregation\n",
    "1. way one sum the predict values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ccrcc     0.9306    0.9617    0.9459       209\n",
      "        prcc     0.9691    0.8920    0.9290       176\n",
      "       chrcc     0.8571    0.9429    0.8980        70\n",
      "\n",
      "    accuracy                         0.9319       455\n",
      "   macro avg     0.9189    0.9322    0.9243       455\n",
      "weighted avg     0.9342    0.9319    0.9320       455\n",
      "\n",
      "[[201   4   4]\n",
      " [ 12 157   7]\n",
      " [  3   1  66]]\n"
     ]
    }
   ],
   "source": [
    "test_set_group_mean = test_set_cancer.groupby(by='slidename', as_index=False).mean()\n",
    "test_set_group_mean['predict'] = np.argmax(np.array(test_set_group_mean[subtypes]), axis=1)\n",
    "print(classification_report(test_set_group_mean['label'], test_set_group_mean['predict'], target_names=subtypes, digits=4))\n",
    "print(confusion_matrix(test_set_group_mean['label'], test_set_group_mean['predict'], labels=[0,1,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. way two sum the predict labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ccrcc     0.9393    0.9617    0.9504       209\n",
      "        prcc     0.9691    0.8920    0.9290       176\n",
      "       chrcc     0.8481    0.9571    0.8993        70\n",
      "\n",
      "    accuracy                         0.9341       455\n",
      "   macro avg     0.9188    0.9370    0.9262       455\n",
      "weighted avg     0.9368    0.9341    0.9342       455\n",
      "\n",
      "[[201   4   4]\n",
      " [ 11 157   8]\n",
      " [  2   1  67]]\n"
     ]
    }
   ],
   "source": [
    "test_set_cancer['predict'] = np.argmax(np.array(test_set_cancer[subtypes]), axis=1)\n",
    "test_set_group_mod = test_set_cancer[['slidename','label','predict']].groupby('slidename', as_index=False).agg(lambda x: x.value_counts().index[0])\n",
    "print(classification_report(test_set_group_mod['label'], test_set_group_mod['predict'], target_names=subtypes, digits=4))\n",
    "print(confusion_matrix(test_set_group_mod['label'], test_set_group_mod['predict'], labels=[0,1,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the AUROC values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(subtypes)):\n",
    "    sub = subtypes[i]\n",
    "    test_set_cancer[sub] = test_set_cancer['predict'].apply(lambda x: 1 if x==i else 0)\n",
    "test_set_cancer['all'] = [1 for i in range(test_set_cancer.shape[0])]\n",
    "test_set_group_predict = test_set_cancer[['slidename','all']+subtypes].groupby(by='slidename', as_index=False).sum()\n",
    "\n",
    "for i in range(len(subtypes)):\n",
    "    sub = subtypes[i]\n",
    "    test_set_group_predict[sub] = test_set_group_predict[sub]/test_set_group_predict['all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "y_one_hot = label_binarize(test_set_group_mod['label'], classes=[0,1,2])\n",
    "y_pred = np.array(test_set_group_predict[subtypes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro AUC: 0.989080\n",
      "Macro AUC: 0.989765\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print(\"Micro AUC: %f\" % roc_auc_score(y_one_hot, y_pred, average='micro'))\n",
    "print(\"Macro AUC: %f\" % roc_auc_score(y_one_hot, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class 1 AUC: 0.984070\n",
      "class 2 AUC: 0.990938\n",
      "class 3 AUC: 0.994286\n"
     ]
    }
   ],
   "source": [
    "print(\"class 1 AUC: %f\" % roc_auc_score(y_one_hot[:,0], y_pred[:,0]))\n",
    "print(\"class 2 AUC: %f\" % roc_auc_score(y_one_hot[:,1], y_pred[:,1]))\n",
    "print(\"class 3 AUC: %f\" % roc_auc_score(y_one_hot[:,2], y_pred[:,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
